{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFmaQ9uuYbDP"
   },
   "source": [
    "# Lab 7: Transformer in Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etCgF-aAiz_H"
   },
   "source": [
    "In this lab, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words. \n",
    "\n",
    "A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the rest of the lab for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "[``nn.TransformerEncoderLayer``](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer). \n",
    "\n",
    "Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. \n",
    "\n",
    "To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Iy2R23Wciz-_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4esKuC8YN5Z"
   },
   "source": [
    "It could be necessary to install torchtext on Google Colab or your personal laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oveLu8vjsjiL",
    "outputId": "c5241bdf-8892-43f2-b6aa-2c5a85a6a2b5"
   },
   "outputs": [],
   "source": [
    "# Sur Google colab\n",
    "#!pip install torchtext\n",
    "# ou avec Anaconda\n",
    "#conda install -c pytorch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adlOFAk4tCfW",
    "outputId": "947ba167-5ceb-4751-cff0-724872741e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTEy4q9Oiz_H"
   },
   "source": [
    "## Define the model\n",
    "----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First steps with the transformer layers\n",
    "\n",
    "\"TransformerEncoderLayer\" is made up of self-attn and feedforward network. This standard encoder layer is based on the paper “Attention Is All You Need”.\n",
    "\n",
    "\"dim_feedforward\" is the dimension of the hidden layer in the feedforward neural network.\n",
    "\n",
    "More details on them [here](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=1000)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)\n",
    "print(src.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer as a stack of encoder layers. \"TransformerEncoder\" is a stack of \"num_layers\" encoder layers.\n",
    "\n",
    "More details on them [here](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32, 512])\n",
      "torch.Size([9, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(9, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "print(src.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the meaning of parameters in \"__init__\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: explain how the \"forward\" function works? What is the role of the mask \"src_mask\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "VvGmcEwgiz_I"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    # Generate an upper triangular mask: lower part is -inf and upper part is 0.0\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # torch.triu returns a copy of a matrix with the elements below the k-th diagonal zeroed.\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src) # add the positional encoding (pe) to src: src <- src + pe\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwmST9ykiz_I"
   },
   "source": [
    "### Positional Encoding\n",
    "\n",
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the role of \"Positional Encoding\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "JjnutiJBiz_I"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the role of the mask \"src_mask\" in the class \"TransformerModel\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH5mfkj1iz_J"
   },
   "source": [
    "Load and batch data\n",
    "-------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTA_RjUGiz_J"
   },
   "source": [
    "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "$$\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "$$\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is TorchText?\n",
    "\n",
    "TorchText is a pytorch package that contains different data processing methods as well as popular NLP datasets. According to the official PyTorch documentation, torchtext has 4 main functionalities: data, datasets, vocab, and utils. Data is mainly used to create custom dataset class, batching samples etc. Datasets consists of the various NLP datasets from sentiment analysis to question answering. Vocab covers different methods of processing text and utils consists of additional helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8ZrwYnrc6ec"
   },
   "source": [
    "#### Warning: path to access the dataset\n",
    "\n",
    "The following variables \"from_path\" and \"to_path\" are necessary to store the dataset. \n",
    "\n",
    "They are defined for Google colab.\n",
    "\n",
    "You must change them if you are using your personal Python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "GyaOM6k7c4sX"
   },
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "#from_path = '/content/sample_data/wikitext-2-v1.zip'\n",
    "#to_path = '/content/sample_data/'\n",
    "\n",
    "# Personal laptop\n",
    "from_path = './wikitext-2-v1.zip'\n",
    "to_path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUd0qpUOdSCE"
   },
   "source": [
    "Download and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "KDD3ZMPixyw9"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "download_from_url(url, from_path)\n",
    "test_filepath, valid_filepath, train_filepath = extract_archive(from_path, to_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSEZ3o1VkixN",
    "outputId": "04d05af0-63db-4740-d38c-86e0be62a412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./wikitext-2/wiki.test.tokens\n",
      "./wikitext-2/wiki.valid.tokens\n",
      "./wikitext-2/wiki.train.tokens\n"
     ]
    }
   ],
   "source": [
    "print(test_filepath)\n",
    "print(valid_filepath)\n",
    "print(train_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K_cwaqPmC5T"
   },
   "source": [
    "### Question: what is the role of \"tokenizer\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the role of \"vocab\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                      iter(io.open(train_filepath,\n",
    "                                                   encoding=\"utf8\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the role of the function \"data_process\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANbVqGvKiz_J",
    "outputId": "b9ee0001-392d-40f2-b04d-ed94440df133"
   },
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2049990])\n",
      "tensor([    9,  3849,  3869,   881,     9, 20000,    83,  3849,    88,     4])\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "# The length of train_data corresponds to the sequence of items that have been encoded.\n",
    "print(train_data.shape)\n",
    "# Print the 10 first first items as numbers\n",
    "print(train_data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the size of the dataset to get results faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2049990])\n",
      "torch.Size([214417])\n",
      "torch.Size([241859])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data = train_data[0:10000]\n",
    "val_data = val_data[0:10000]\n",
    "test_data = test_data[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtDYy5DikZPm"
   },
   "source": [
    "Use a GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Lbxzg8VMkOlV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym8sbZC3kWAm"
   },
   "source": [
    "Create the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the physical meaning (with respect to the raw text) of a batch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "pHhzTJ2FkQM-"
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz # number of elements in a batch\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    # \"contiguous()\" returns a contiguous in memory tensor containing the same data as self tensor. \n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20 # number of batches\n",
    "eval_batch_size = 10\n",
    "\n",
    "import copy \n",
    "train_data_initial = copy.deepcopy(train_data)\n",
    "\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0NPamPsiz_K"
   },
   "source": [
    "## Functions to generate input and target sequence\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VHdaLV1iz_K"
   },
   "source": [
    "``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/transformer_input_target.png)\n",
    "\n",
    "It means that from each column of \"Input\" (a column is a sequence), we want to predict the column of \"Target\".\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is returned by \"get_batch\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nV8xDmYLiz_K"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdtYDQLfiz_L"
   },
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pknl3SUDiz_L"
   },
   "source": [
    "The model is set up with the hyperparameter below. The vocab size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "-OH8RSa6iz_L"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab.get_stoi()) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension (it corresponds to \"ninp\")\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIbuuW6niz_M"
   },
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncoyLgh0iz_N"
   },
   "source": [
    "[``CrossEntropyLoss``](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) is applied to track the loss and\n",
    "[``SGD``](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD)\n",
    "implements stochastic gradient descent method as the optimizer.\n",
    "\n",
    "The initial learning rate is set to 5.0. [``StepLR``](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR) is\n",
    "applied to adjust the learn rate through epochs. \n",
    "\n",
    "During the\n",
    "training, we use\n",
    "[``nn.utils.clip_grad_norm_``](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_)\n",
    "function to scale all the gradient together to prevent exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the role of \"StepLR\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "FOu7E0KOhx39"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is an evaluation criterion that has been well studied over the past few years\n",
    "\n",
    "Perplexity, called ppl in the next cell, is the exponentiation of the average cross entropy of a corpus (Mikolov et al., 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the role of \"torch.nn.utils.clip_grad_norm_\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "9e4uRgKjiz_N"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 20\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    # Loop over the training batches\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print a summary each log_interval iterations\n",
    "        # The summary is focused on the loss \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval # compute the current loss over the log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function evaluates a trained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "byPVmsBBhs6S"
   },
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-xqhHcBiz_O"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "lDDZSKD_iz_O",
    "outputId": "e52b83d0-b87d-44f1-fc9b-2e4c280045d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    20/ 2928 batches | lr 5.00 | ms/batch 22.25 | loss 10.73 | ppl 45787.60\n",
      "| epoch   1 |    40/ 2928 batches | lr 5.00 | ms/batch 17.40 | loss  9.36 | ppl 11601.78\n",
      "| epoch   1 |    60/ 2928 batches | lr 5.00 | ms/batch 16.85 | loss  8.61 | ppl  5462.68\n",
      "| epoch   1 |    80/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  8.06 | ppl  3179.35\n",
      "| epoch   1 |   100/ 2928 batches | lr 5.00 | ms/batch 17.02 | loss  7.88 | ppl  2636.28\n",
      "| epoch   1 |   120/ 2928 batches | lr 5.00 | ms/batch 17.00 | loss  7.66 | ppl  2112.96\n",
      "| epoch   1 |   140/ 2928 batches | lr 5.00 | ms/batch 16.80 | loss  7.61 | ppl  2025.87\n",
      "| epoch   1 |   160/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  7.36 | ppl  1572.95\n",
      "| epoch   1 |   180/ 2928 batches | lr 5.00 | ms/batch 16.80 | loss  7.30 | ppl  1479.28\n",
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  7.20 | ppl  1334.86\n",
      "| epoch   1 |   220/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  7.11 | ppl  1226.22\n",
      "| epoch   1 |   240/ 2928 batches | lr 5.00 | ms/batch 16.88 | loss  7.05 | ppl  1153.73\n",
      "| epoch   1 |   260/ 2928 batches | lr 5.00 | ms/batch 17.35 | loss  7.04 | ppl  1142.89\n",
      "| epoch   1 |   280/ 2928 batches | lr 5.00 | ms/batch 16.45 | loss  7.03 | ppl  1125.63\n",
      "| epoch   1 |   300/ 2928 batches | lr 5.00 | ms/batch 17.20 | loss  6.92 | ppl  1014.48\n",
      "| epoch   1 |   320/ 2928 batches | lr 5.00 | ms/batch 17.15 | loss  6.80 | ppl   897.62\n",
      "| epoch   1 |   340/ 2928 batches | lr 5.00 | ms/batch 16.90 | loss  6.76 | ppl   861.72\n",
      "| epoch   1 |   360/ 2928 batches | lr 5.00 | ms/batch 16.81 | loss  6.73 | ppl   840.45\n",
      "| epoch   1 |   380/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.78 | ppl   876.56\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 17.41 | loss  6.64 | ppl   762.38\n",
      "| epoch   1 |   420/ 2928 batches | lr 5.00 | ms/batch 17.08 | loss  6.47 | ppl   643.01\n",
      "| epoch   1 |   440/ 2928 batches | lr 5.00 | ms/batch 17.25 | loss  6.51 | ppl   668.52\n",
      "| epoch   1 |   460/ 2928 batches | lr 5.00 | ms/batch 16.90 | loss  6.50 | ppl   668.40\n",
      "| epoch   1 |   480/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.59 | ppl   726.97\n",
      "| epoch   1 |   500/ 2928 batches | lr 5.00 | ms/batch 16.80 | loss  6.55 | ppl   696.01\n",
      "| epoch   1 |   520/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.40 | ppl   603.35\n",
      "| epoch   1 |   540/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.41 | ppl   608.02\n",
      "| epoch   1 |   560/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.27 | ppl   526.54\n",
      "| epoch   1 |   580/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.31 | ppl   548.00\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.36 | ppl   578.20\n",
      "| epoch   1 |   620/ 2928 batches | lr 5.00 | ms/batch 16.90 | loss  6.37 | ppl   582.42\n",
      "| epoch   1 |   640/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.39 | ppl   597.51\n",
      "| epoch   1 |   660/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.32 | ppl   557.11\n",
      "| epoch   1 |   680/ 2928 batches | lr 5.00 | ms/batch 16.30 | loss  6.32 | ppl   553.23\n",
      "| epoch   1 |   700/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  6.23 | ppl   505.99\n",
      "| epoch   1 |   720/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.24 | ppl   515.05\n",
      "| epoch   1 |   740/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.26 | ppl   522.81\n",
      "| epoch   1 |   760/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.35 | ppl   572.35\n",
      "| epoch   1 |   780/ 2928 batches | lr 5.00 | ms/batch 16.78 | loss  6.29 | ppl   541.79\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.23 | ppl   508.12\n",
      "| epoch   1 |   820/ 2928 batches | lr 5.00 | ms/batch 17.20 | loss  6.23 | ppl   506.01\n",
      "| epoch   1 |   840/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.19 | ppl   487.74\n",
      "| epoch   1 |   860/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.17 | ppl   480.10\n",
      "| epoch   1 |   880/ 2928 batches | lr 5.00 | ms/batch 16.20 | loss  6.18 | ppl   483.45\n",
      "| epoch   1 |   900/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  6.23 | ppl   505.57\n",
      "| epoch   1 |   920/ 2928 batches | lr 5.00 | ms/batch 16.85 | loss  6.27 | ppl   526.72\n",
      "| epoch   1 |   940/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.15 | ppl   466.38\n",
      "| epoch   1 |   960/ 2928 batches | lr 5.00 | ms/batch 16.95 | loss  6.10 | ppl   444.53\n",
      "| epoch   1 |   980/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.20 | ppl   490.58\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.16 | ppl   473.10\n",
      "| epoch   1 |  1020/ 2928 batches | lr 5.00 | ms/batch 16.20 | loss  6.27 | ppl   526.00\n",
      "| epoch   1 |  1040/ 2928 batches | lr 5.00 | ms/batch 16.45 | loss  6.17 | ppl   479.14\n",
      "| epoch   1 |  1060/ 2928 batches | lr 5.00 | ms/batch 16.34 | loss  6.09 | ppl   440.79\n",
      "| epoch   1 |  1080/ 2928 batches | lr 5.00 | ms/batch 16.85 | loss  6.13 | ppl   460.63\n",
      "| epoch   1 |  1100/ 2928 batches | lr 5.00 | ms/batch 16.73 | loss  6.12 | ppl   453.79\n",
      "| epoch   1 |  1120/ 2928 batches | lr 5.00 | ms/batch 16.30 | loss  6.07 | ppl   433.09\n",
      "| epoch   1 |  1140/ 2928 batches | lr 5.00 | ms/batch 16.30 | loss  6.18 | ppl   483.49\n",
      "| epoch   1 |  1160/ 2928 batches | lr 5.00 | ms/batch 16.15 | loss  6.21 | ppl   499.88\n",
      "| epoch   1 |  1180/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  6.15 | ppl   466.48\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 16.80 | loss  6.18 | ppl   480.98\n",
      "| epoch   1 |  1220/ 2928 batches | lr 5.00 | ms/batch 16.58 | loss  6.10 | ppl   447.85\n",
      "| epoch   1 |  1240/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.21 | ppl   497.34\n",
      "| epoch   1 |  1260/ 2928 batches | lr 5.00 | ms/batch 16.74 | loss  6.14 | ppl   462.87\n",
      "| epoch   1 |  1280/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  6.19 | ppl   487.51\n",
      "| epoch   1 |  1300/ 2928 batches | lr 5.00 | ms/batch 16.45 | loss  6.15 | ppl   467.74\n",
      "| epoch   1 |  1320/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  6.06 | ppl   429.13\n",
      "| epoch   1 |  1340/ 2928 batches | lr 5.00 | ms/batch 16.73 | loss  6.02 | ppl   413.16\n",
      "| epoch   1 |  1360/ 2928 batches | lr 5.00 | ms/batch 16.62 | loss  6.06 | ppl   429.78\n",
      "| epoch   1 |  1380/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.09 | ppl   440.84\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.14 | ppl   463.87\n",
      "| epoch   1 |  1420/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.12 | ppl   455.83\n",
      "| epoch   1 |  1440/ 2928 batches | lr 5.00 | ms/batch 16.36 | loss  6.12 | ppl   455.49\n",
      "| epoch   1 |  1460/ 2928 batches | lr 5.00 | ms/batch 16.15 | loss  6.24 | ppl   513.94\n",
      "| epoch   1 |  1480/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.16 | ppl   474.55\n",
      "| epoch   1 |  1500/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.11 | ppl   449.71\n",
      "| epoch   1 |  1520/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.12 | ppl   455.23\n",
      "| epoch   1 |  1540/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  6.03 | ppl   414.88\n",
      "| epoch   1 |  1560/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.06 | ppl   427.38\n",
      "| epoch   1 |  1580/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.04 | ppl   419.53\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.07 | ppl   431.51\n",
      "| epoch   1 |  1620/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  6.03 | ppl   415.35\n",
      "| epoch   1 |  1640/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.97 | ppl   390.53\n",
      "| epoch   1 |  1660/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.10 | ppl   443.91\n",
      "| epoch   1 |  1680/ 2928 batches | lr 5.00 | ms/batch 16.30 | loss  5.98 | ppl   396.45\n",
      "| epoch   1 |  1700/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  6.05 | ppl   424.02\n",
      "| epoch   1 |  1720/ 2928 batches | lr 5.00 | ms/batch 16.25 | loss  6.00 | ppl   404.42\n",
      "| epoch   1 |  1740/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  6.02 | ppl   413.33\n",
      "| epoch   1 |  1760/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  6.05 | ppl   424.33\n",
      "| epoch   1 |  1780/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.02 | ppl   409.56\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.06 | ppl   430.16\n",
      "| epoch   1 |  1820/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  6.12 | ppl   453.71\n",
      "| epoch   1 |  1840/ 2928 batches | lr 5.00 | ms/batch 16.76 | loss  6.02 | ppl   413.08\n",
      "| epoch   1 |  1860/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  6.02 | ppl   410.64\n",
      "| epoch   1 |  1880/ 2928 batches | lr 5.00 | ms/batch 16.25 | loss  5.97 | ppl   390.80\n",
      "| epoch   1 |  1900/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.98 | ppl   393.74\n",
      "| epoch   1 |  1920/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  6.04 | ppl   421.76\n",
      "| epoch   1 |  1940/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.99 | ppl   398.52\n",
      "| epoch   1 |  1960/ 2928 batches | lr 5.00 | ms/batch 16.25 | loss  5.96 | ppl   388.13\n",
      "| epoch   1 |  1980/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  6.08 | ppl   435.55\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.96 | ppl   386.52\n",
      "| epoch   1 |  2020/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  5.91 | ppl   369.87\n",
      "| epoch   1 |  2040/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  6.00 | ppl   402.94\n",
      "| epoch   1 |  2060/ 2928 batches | lr 5.00 | ms/batch 17.10 | loss  5.89 | ppl   360.52\n",
      "| epoch   1 |  2080/ 2928 batches | lr 5.00 | ms/batch 16.45 | loss  5.89 | ppl   361.18\n",
      "| epoch   1 |  2100/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.85 | ppl   345.55\n",
      "| epoch   1 |  2120/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  5.89 | ppl   361.38\n",
      "| epoch   1 |  2140/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.91 | ppl   368.69\n",
      "| epoch   1 |  2160/ 2928 batches | lr 5.00 | ms/batch 16.25 | loss  5.83 | ppl   340.63\n",
      "| epoch   1 |  2180/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.86 | ppl   351.49\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 16.69 | loss  5.89 | ppl   360.81\n",
      "| epoch   1 |  2220/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  5.97 | ppl   391.81\n",
      "| epoch   1 |  2240/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.97 | ppl   391.37\n",
      "| epoch   1 |  2260/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  5.95 | ppl   383.38\n",
      "| epoch   1 |  2280/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.00 | ppl   405.41\n",
      "| epoch   1 |  2300/ 2928 batches | lr 5.00 | ms/batch 17.00 | loss  6.07 | ppl   431.42\n",
      "| epoch   1 |  2320/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  5.99 | ppl   399.90\n",
      "| epoch   1 |  2340/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  6.00 | ppl   402.44\n",
      "| epoch   1 |  2360/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  5.96 | ppl   386.50\n",
      "| epoch   1 |  2380/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.91 | ppl   369.77\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.93 | ppl   376.97\n",
      "| epoch   1 |  2420/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  6.01 | ppl   406.08\n",
      "| epoch   1 |  2440/ 2928 batches | lr 5.00 | ms/batch 16.90 | loss  5.94 | ppl   380.54\n",
      "| epoch   1 |  2460/ 2928 batches | lr 5.00 | ms/batch 17.10 | loss  5.94 | ppl   378.93\n",
      "| epoch   1 |  2480/ 2928 batches | lr 5.00 | ms/batch 17.35 | loss  5.95 | ppl   383.04\n",
      "| epoch   1 |  2500/ 2928 batches | lr 5.00 | ms/batch 17.00 | loss  6.02 | ppl   413.24\n",
      "| epoch   1 |  2520/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  6.02 | ppl   409.92\n",
      "| epoch   1 |  2540/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.99 | ppl   399.70\n",
      "| epoch   1 |  2560/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.93 | ppl   375.05\n",
      "| epoch   1 |  2580/ 2928 batches | lr 5.00 | ms/batch 16.35 | loss  5.95 | ppl   384.06\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 16.40 | loss  5.82 | ppl   336.57\n",
      "| epoch   1 |  2620/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  5.84 | ppl   343.55\n",
      "| epoch   1 |  2640/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  5.79 | ppl   328.02\n",
      "| epoch   1 |  2660/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  5.86 | ppl   351.84\n",
      "| epoch   1 |  2680/ 2928 batches | lr 5.00 | ms/batch 16.90 | loss  5.92 | ppl   372.06\n",
      "| epoch   1 |  2700/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.90 | ppl   366.82\n",
      "| epoch   1 |  2720/ 2928 batches | lr 5.00 | ms/batch 16.45 | loss  5.88 | ppl   356.64\n",
      "| epoch   1 |  2740/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  5.91 | ppl   369.46\n",
      "| epoch   1 |  2760/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.83 | ppl   341.60\n",
      "| epoch   1 |  2780/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  5.90 | ppl   365.21\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 16.70 | loss  6.01 | ppl   407.24\n",
      "| epoch   1 |  2820/ 2928 batches | lr 5.00 | ms/batch 16.60 | loss  5.88 | ppl   358.18\n",
      "| epoch   1 |  2840/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.79 | ppl   327.33\n",
      "| epoch   1 |  2860/ 2928 batches | lr 5.00 | ms/batch 16.55 | loss  5.75 | ppl   314.68\n",
      "| epoch   1 |  2880/ 2928 batches | lr 5.00 | ms/batch 16.65 | loss  5.80 | ppl   329.29\n",
      "| epoch   1 |  2900/ 2928 batches | lr 5.00 | ms/batch 16.75 | loss  5.75 | ppl   315.29\n",
      "| epoch   1 |  2920/ 2928 batches | lr 5.00 | ms/batch 16.50 | loss  5.78 | ppl   323.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 51.23s | valid loss  5.82 | valid ppl   336.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    20/ 2928 batches | lr 4.75 | ms/batch 18.25 | loss  6.15 | ppl   468.39\n",
      "| epoch   2 |    40/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.97 | ppl   392.38\n",
      "| epoch   2 |    60/ 2928 batches | lr 4.75 | ms/batch 16.68 | loss  5.84 | ppl   343.25\n",
      "| epoch   2 |    80/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.82 | ppl   338.27\n",
      "| epoch   2 |   100/ 2928 batches | lr 4.75 | ms/batch 16.80 | loss  5.86 | ppl   350.18\n",
      "| epoch   2 |   120/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.83 | ppl   340.80\n",
      "| epoch   2 |   140/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.80 | ppl   329.66\n",
      "| epoch   2 |   160/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.85 | ppl   346.94\n",
      "| epoch   2 |   180/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.81 | ppl   332.15\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.74 | ppl   310.25\n",
      "| epoch   2 |   220/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.80 | ppl   331.17\n",
      "| epoch   2 |   240/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.89 | ppl   361.52\n",
      "| epoch   2 |   260/ 2928 batches | lr 4.75 | ms/batch 16.80 | loss  5.84 | ppl   344.08\n",
      "| epoch   2 |   280/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.93 | ppl   376.55\n",
      "| epoch   2 |   300/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.91 | ppl   367.32\n",
      "| epoch   2 |   320/ 2928 batches | lr 4.75 | ms/batch 16.00 | loss  5.87 | ppl   355.20\n",
      "| epoch   2 |   340/ 2928 batches | lr 4.75 | ms/batch 16.49 | loss  5.87 | ppl   353.02\n",
      "| epoch   2 |   360/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.83 | ppl   339.88\n",
      "| epoch   2 |   380/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.86 | ppl   350.38\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 16.40 | loss  5.77 | ppl   320.30\n",
      "| epoch   2 |   420/ 2928 batches | lr 4.75 | ms/batch 16.35 | loss  5.62 | ppl   277.03\n",
      "| epoch   2 |   440/ 2928 batches | lr 4.75 | ms/batch 16.40 | loss  5.67 | ppl   289.38\n",
      "| epoch   2 |   460/ 2928 batches | lr 4.75 | ms/batch 17.00 | loss  5.74 | ppl   311.55\n",
      "| epoch   2 |   480/ 2928 batches | lr 4.75 | ms/batch 16.80 | loss  5.82 | ppl   337.78\n",
      "| epoch   2 |   500/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.68 | ppl   294.12\n",
      "| epoch   2 |   520/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.66 | ppl   285.99\n",
      "| epoch   2 |   540/ 2928 batches | lr 4.75 | ms/batch 16.30 | loss  5.67 | ppl   289.61\n",
      "| epoch   2 |   560/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.51 | ppl   248.37\n",
      "| epoch   2 |   580/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.63 | ppl   278.20\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.66 | ppl   288.02\n",
      "| epoch   2 |   620/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.67 | ppl   290.38\n",
      "| epoch   2 |   640/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.75 | ppl   312.78\n",
      "| epoch   2 |   660/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.68 | ppl   292.52\n",
      "| epoch   2 |   680/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.71 | ppl   302.99\n",
      "| epoch   2 |   700/ 2928 batches | lr 4.75 | ms/batch 16.33 | loss  5.66 | ppl   287.26\n",
      "| epoch   2 |   720/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.69 | ppl   296.06\n",
      "| epoch   2 |   740/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.69 | ppl   296.89\n",
      "| epoch   2 |   760/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.78 | ppl   322.16\n",
      "| epoch   2 |   780/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.73 | ppl   307.70\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.71 | ppl   302.22\n",
      "| epoch   2 |   820/ 2928 batches | lr 4.75 | ms/batch 16.10 | loss  5.69 | ppl   296.18\n",
      "| epoch   2 |   840/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.68 | ppl   291.86\n",
      "| epoch   2 |   860/ 2928 batches | lr 4.75 | ms/batch 16.35 | loss  5.64 | ppl   281.04\n",
      "| epoch   2 |   880/ 2928 batches | lr 4.75 | ms/batch 16.40 | loss  5.65 | ppl   283.42\n",
      "| epoch   2 |   900/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.66 | ppl   287.38\n",
      "| epoch   2 |   920/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.72 | ppl   303.88\n",
      "| epoch   2 |   940/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.63 | ppl   278.18\n",
      "| epoch   2 |   960/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.60 | ppl   271.38\n",
      "| epoch   2 |   980/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.67 | ppl   289.70\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.66 | ppl   287.12\n",
      "| epoch   2 |  1020/ 2928 batches | lr 4.75 | ms/batch 16.96 | loss  5.76 | ppl   318.33\n",
      "| epoch   2 |  1040/ 2928 batches | lr 4.75 | ms/batch 16.44 | loss  5.66 | ppl   287.62\n",
      "| epoch   2 |  1060/ 2928 batches | lr 4.75 | ms/batch 17.35 | loss  5.60 | ppl   270.82\n",
      "| epoch   2 |  1080/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.63 | ppl   277.71\n",
      "| epoch   2 |  1100/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.63 | ppl   279.29\n",
      "| epoch   2 |  1120/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  5.60 | ppl   270.69\n",
      "| epoch   2 |  1140/ 2928 batches | lr 4.75 | ms/batch 15.90 | loss  5.72 | ppl   304.82\n",
      "| epoch   2 |  1160/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.74 | ppl   311.81\n",
      "| epoch   2 |  1180/ 2928 batches | lr 4.75 | ms/batch 16.75 | loss  5.69 | ppl   296.94\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 16.42 | loss  5.76 | ppl   318.32\n",
      "| epoch   2 |  1220/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.66 | ppl   287.51\n",
      "| epoch   2 |  1240/ 2928 batches | lr 4.75 | ms/batch 15.95 | loss  5.78 | ppl   324.41\n",
      "| epoch   2 |  1260/ 2928 batches | lr 4.75 | ms/batch 15.95 | loss  5.73 | ppl   306.51\n",
      "| epoch   2 |  1280/ 2928 batches | lr 4.75 | ms/batch 15.95 | loss  5.80 | ppl   329.77\n",
      "| epoch   2 |  1300/ 2928 batches | lr 4.75 | ms/batch 16.05 | loss  5.72 | ppl   305.62\n",
      "| epoch   2 |  1320/ 2928 batches | lr 4.75 | ms/batch 15.90 | loss  5.65 | ppl   283.22\n",
      "| epoch   2 |  1340/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  5.60 | ppl   271.46\n",
      "| epoch   2 |  1360/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.64 | ppl   280.31\n",
      "| epoch   2 |  1380/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.67 | ppl   289.68\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 15.90 | loss  5.72 | ppl   306.40\n",
      "| epoch   2 |  1420/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.73 | ppl   308.56\n",
      "| epoch   2 |  1440/ 2928 batches | lr 4.75 | ms/batch 16.00 | loss  5.73 | ppl   308.23\n",
      "| epoch   2 |  1460/ 2928 batches | lr 4.75 | ms/batch 15.90 | loss  5.85 | ppl   346.05\n",
      "| epoch   2 |  1480/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  5.74 | ppl   311.35\n",
      "| epoch   2 |  1500/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.69 | ppl   295.03\n",
      "| epoch   2 |  1520/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.72 | ppl   305.58\n",
      "| epoch   2 |  1540/ 2928 batches | lr 4.75 | ms/batch 16.05 | loss  5.65 | ppl   284.25\n",
      "| epoch   2 |  1560/ 2928 batches | lr 4.75 | ms/batch 15.95 | loss  5.67 | ppl   289.80\n",
      "| epoch   2 |  1580/ 2928 batches | lr 4.75 | ms/batch 16.35 | loss  5.67 | ppl   290.21\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.67 | ppl   288.91\n",
      "| epoch   2 |  1620/ 2928 batches | lr 4.75 | ms/batch 16.00 | loss  5.63 | ppl   278.07\n",
      "| epoch   2 |  1640/ 2928 batches | lr 4.75 | ms/batch 16.05 | loss  5.59 | ppl   267.51\n",
      "| epoch   2 |  1660/ 2928 batches | lr 4.75 | ms/batch 16.05 | loss  5.71 | ppl   301.34\n",
      "| epoch   2 |  1680/ 2928 batches | lr 4.75 | ms/batch 16.40 | loss  5.60 | ppl   270.06\n",
      "| epoch   2 |  1700/ 2928 batches | lr 4.75 | ms/batch 16.30 | loss  5.68 | ppl   292.98\n",
      "| epoch   2 |  1720/ 2928 batches | lr 4.75 | ms/batch 17.25 | loss  5.65 | ppl   283.50\n",
      "| epoch   2 |  1740/ 2928 batches | lr 4.75 | ms/batch 17.00 | loss  5.66 | ppl   286.59\n",
      "| epoch   2 |  1760/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.69 | ppl   295.86\n",
      "| epoch   2 |  1780/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.66 | ppl   287.50\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.71 | ppl   301.93\n",
      "| epoch   2 |  1820/ 2928 batches | lr 4.75 | ms/batch 16.72 | loss  5.80 | ppl   329.83\n",
      "| epoch   2 |  1840/ 2928 batches | lr 4.75 | ms/batch 16.86 | loss  5.69 | ppl   297.31\n",
      "| epoch   2 |  1860/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.67 | ppl   288.83\n",
      "| epoch   2 |  1880/ 2928 batches | lr 4.75 | ms/batch 16.32 | loss  5.63 | ppl   278.29\n",
      "| epoch   2 |  1900/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.65 | ppl   285.11\n",
      "| epoch   2 |  1920/ 2928 batches | lr 4.75 | ms/batch 16.95 | loss  5.69 | ppl   297.30\n",
      "| epoch   2 |  1940/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  5.65 | ppl   284.13\n",
      "| epoch   2 |  1960/ 2928 batches | lr 4.75 | ms/batch 16.80 | loss  5.62 | ppl   275.36\n",
      "| epoch   2 |  1980/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.74 | ppl   311.11\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 16.30 | loss  5.61 | ppl   273.61\n",
      "| epoch   2 |  2020/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  5.56 | ppl   258.93\n",
      "| epoch   2 |  2040/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.64 | ppl   282.17\n",
      "| epoch   2 |  2060/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.51 | ppl   248.24\n",
      "| epoch   2 |  2080/ 2928 batches | lr 4.75 | ms/batch 16.40 | loss  5.55 | ppl   256.66\n",
      "| epoch   2 |  2100/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.52 | ppl   250.16\n",
      "| epoch   2 |  2120/ 2928 batches | lr 4.75 | ms/batch 16.35 | loss  5.59 | ppl   267.71\n",
      "| epoch   2 |  2140/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.59 | ppl   267.95\n",
      "| epoch   2 |  2160/ 2928 batches | lr 4.75 | ms/batch 17.05 | loss  5.52 | ppl   249.71\n",
      "| epoch   2 |  2180/ 2928 batches | lr 4.75 | ms/batch 16.60 | loss  5.56 | ppl   258.90\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 16.90 | loss  5.56 | ppl   259.33\n",
      "| epoch   2 |  2220/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.66 | ppl   286.38\n",
      "| epoch   2 |  2240/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.64 | ppl   281.12\n",
      "| epoch   2 |  2260/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.63 | ppl   279.60\n",
      "| epoch   2 |  2280/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.69 | ppl   296.14\n",
      "| epoch   2 |  2300/ 2928 batches | lr 4.75 | ms/batch 16.92 | loss  5.75 | ppl   313.93\n",
      "| epoch   2 |  2320/ 2928 batches | lr 4.75 | ms/batch 17.30 | loss  5.68 | ppl   294.32\n",
      "| epoch   2 |  2340/ 2928 batches | lr 4.75 | ms/batch 16.85 | loss  5.69 | ppl   295.34\n",
      "| epoch   2 |  2360/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.59 | ppl   267.15\n",
      "| epoch   2 |  2380/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.54 | ppl   255.46\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.59 | ppl   267.35\n",
      "| epoch   2 |  2420/ 2928 batches | lr 4.75 | ms/batch 16.10 | loss  5.67 | ppl   289.61\n",
      "| epoch   2 |  2440/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  5.63 | ppl   277.92\n",
      "| epoch   2 |  2460/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  5.59 | ppl   266.92\n",
      "| epoch   2 |  2480/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  5.65 | ppl   285.24\n",
      "| epoch   2 |  2500/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  5.72 | ppl   304.83\n",
      "| epoch   2 |  2520/ 2928 batches | lr 4.75 | ms/batch 16.32 | loss  5.73 | ppl   306.45\n",
      "| epoch   2 |  2540/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.70 | ppl   298.53\n",
      "| epoch   2 |  2560/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  5.62 | ppl   276.46\n",
      "| epoch   2 |  2580/ 2928 batches | lr 4.75 | ms/batch 16.35 | loss  5.61 | ppl   272.89\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.54 | ppl   254.79\n",
      "| epoch   2 |  2620/ 2928 batches | lr 4.75 | ms/batch 16.17 | loss  5.55 | ppl   256.82\n",
      "| epoch   2 |  2640/ 2928 batches | lr 4.75 | ms/batch 16.17 | loss  5.52 | ppl   249.43\n",
      "| epoch   2 |  2660/ 2928 batches | lr 4.75 | ms/batch 16.55 | loss  5.55 | ppl   258.52\n",
      "| epoch   2 |  2680/ 2928 batches | lr 4.75 | ms/batch 16.05 | loss  5.62 | ppl   277.25\n",
      "| epoch   2 |  2700/ 2928 batches | lr 4.75 | ms/batch 16.10 | loss  5.60 | ppl   270.89\n",
      "| epoch   2 |  2720/ 2928 batches | lr 4.75 | ms/batch 16.30 | loss  5.59 | ppl   266.63\n",
      "| epoch   2 |  2740/ 2928 batches | lr 4.75 | ms/batch 16.95 | loss  5.60 | ppl   269.75\n",
      "| epoch   2 |  2760/ 2928 batches | lr 4.75 | ms/batch 16.36 | loss  5.54 | ppl   254.48\n",
      "| epoch   2 |  2780/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.58 | ppl   266.31\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.68 | ppl   292.34\n",
      "| epoch   2 |  2820/ 2928 batches | lr 4.75 | ms/batch 16.65 | loss  5.58 | ppl   263.88\n",
      "| epoch   2 |  2840/ 2928 batches | lr 4.75 | ms/batch 16.70 | loss  5.48 | ppl   239.95\n",
      "| epoch   2 |  2860/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.43 | ppl   228.54\n",
      "| epoch   2 |  2880/ 2928 batches | lr 4.75 | ms/batch 16.50 | loss  5.47 | ppl   238.10\n",
      "| epoch   2 |  2900/ 2928 batches | lr 4.75 | ms/batch 16.25 | loss  5.48 | ppl   240.63\n",
      "| epoch   2 |  2920/ 2928 batches | lr 4.75 | ms/batch 16.45 | loss  5.49 | ppl   243.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 50.62s | valid loss  5.67 | valid ppl   289.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    20/ 2928 batches | lr 4.51 | ms/batch 17.15 | loss  5.88 | ppl   359.05\n",
      "| epoch   3 |    40/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.68 | ppl   291.68\n",
      "| epoch   3 |    60/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.54 | ppl   255.55\n",
      "| epoch   3 |    80/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.58 | ppl   265.62\n",
      "| epoch   3 |   100/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.62 | ppl   276.00\n",
      "| epoch   3 |   120/ 2928 batches | lr 4.51 | ms/batch 17.10 | loss  5.56 | ppl   260.11\n",
      "| epoch   3 |   140/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.52 | ppl   249.25\n",
      "| epoch   3 |   160/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.62 | ppl   275.87\n",
      "| epoch   3 |   180/ 2928 batches | lr 4.51 | ms/batch 16.95 | loss  5.53 | ppl   251.60\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 17.20 | loss  5.52 | ppl   248.77\n",
      "| epoch   3 |   220/ 2928 batches | lr 4.51 | ms/batch 17.80 | loss  5.56 | ppl   260.04\n",
      "| epoch   3 |   240/ 2928 batches | lr 4.51 | ms/batch 17.55 | loss  5.64 | ppl   282.62\n",
      "| epoch   3 |   260/ 2928 batches | lr 4.51 | ms/batch 18.25 | loss  5.60 | ppl   270.05\n",
      "| epoch   3 |   280/ 2928 batches | lr 4.51 | ms/batch 17.80 | loss  5.70 | ppl   299.45\n",
      "| epoch   3 |   300/ 2928 batches | lr 4.51 | ms/batch 16.90 | loss  5.68 | ppl   294.33\n",
      "| epoch   3 |   320/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.67 | ppl   290.74\n",
      "| epoch   3 |   340/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.65 | ppl   283.76\n",
      "| epoch   3 |   360/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.60 | ppl   270.22\n",
      "| epoch   3 |   380/ 2928 batches | lr 4.51 | ms/batch 16.85 | loss  5.62 | ppl   274.99\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.50 | ppl   244.97\n",
      "| epoch   3 |   420/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.40 | ppl   220.45\n",
      "| epoch   3 |   440/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.38 | ppl   217.88\n",
      "| epoch   3 |   460/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.51 | ppl   247.98\n",
      "| epoch   3 |   480/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.57 | ppl   262.24\n",
      "| epoch   3 |   500/ 2928 batches | lr 4.51 | ms/batch 16.66 | loss  5.41 | ppl   222.56\n",
      "| epoch   3 |   520/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.39 | ppl   218.59\n",
      "| epoch   3 |   540/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.42 | ppl   225.77\n",
      "| epoch   3 |   560/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.26 | ppl   193.02\n",
      "| epoch   3 |   580/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.37 | ppl   215.52\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.44 | ppl   231.37\n",
      "| epoch   3 |   620/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.41 | ppl   224.69\n",
      "| epoch   3 |   640/ 2928 batches | lr 4.51 | ms/batch 17.30 | loss  5.53 | ppl   251.17\n",
      "| epoch   3 |   660/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.44 | ppl   230.74\n",
      "| epoch   3 |   680/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.54 | ppl   254.92\n",
      "| epoch   3 |   700/ 2928 batches | lr 4.51 | ms/batch 16.47 | loss  5.43 | ppl   228.38\n",
      "| epoch   3 |   720/ 2928 batches | lr 4.51 | ms/batch 16.85 | loss  5.46 | ppl   236.22\n",
      "| epoch   3 |   740/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.49 | ppl   242.18\n",
      "| epoch   3 |   760/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.55 | ppl   257.71\n",
      "| epoch   3 |   780/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.52 | ppl   248.75\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.49 | ppl   241.57\n",
      "| epoch   3 |   820/ 2928 batches | lr 4.51 | ms/batch 15.95 | loss  5.47 | ppl   237.13\n",
      "| epoch   3 |   840/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.47 | ppl   238.36\n",
      "| epoch   3 |   860/ 2928 batches | lr 4.51 | ms/batch 16.10 | loss  5.44 | ppl   231.26\n",
      "| epoch   3 |   880/ 2928 batches | lr 4.51 | ms/batch 15.85 | loss  5.44 | ppl   230.95\n",
      "| epoch   3 |   900/ 2928 batches | lr 4.51 | ms/batch 16.15 | loss  5.42 | ppl   225.06\n",
      "| epoch   3 |   920/ 2928 batches | lr 4.51 | ms/batch 16.00 | loss  5.50 | ppl   245.66\n",
      "| epoch   3 |   940/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.42 | ppl   226.82\n",
      "| epoch   3 |   960/ 2928 batches | lr 4.51 | ms/batch 16.00 | loss  5.40 | ppl   220.62\n",
      "| epoch   3 |   980/ 2928 batches | lr 4.51 | ms/batch 15.90 | loss  5.42 | ppl   226.24\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 16.10 | loss  5.46 | ppl   234.35\n",
      "| epoch   3 |  1020/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.57 | ppl   261.29\n",
      "| epoch   3 |  1040/ 2928 batches | lr 4.51 | ms/batch 16.05 | loss  5.47 | ppl   236.80\n",
      "| epoch   3 |  1060/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.40 | ppl   220.55\n",
      "| epoch   3 |  1080/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.42 | ppl   225.33\n",
      "| epoch   3 |  1100/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.43 | ppl   228.65\n",
      "| epoch   3 |  1120/ 2928 batches | lr 4.51 | ms/batch 15.95 | loss  5.39 | ppl   219.02\n",
      "| epoch   3 |  1140/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.52 | ppl   249.89\n",
      "| epoch   3 |  1160/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.56 | ppl   259.92\n",
      "| epoch   3 |  1180/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.50 | ppl   245.58\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.57 | ppl   262.20\n",
      "| epoch   3 |  1220/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.47 | ppl   237.96\n",
      "| epoch   3 |  1240/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.58 | ppl   264.71\n",
      "| epoch   3 |  1260/ 2928 batches | lr 4.51 | ms/batch 16.80 | loss  5.54 | ppl   254.49\n",
      "| epoch   3 |  1280/ 2928 batches | lr 4.51 | ms/batch 17.30 | loss  5.57 | ppl   262.86\n",
      "| epoch   3 |  1300/ 2928 batches | lr 4.51 | ms/batch 16.85 | loss  5.51 | ppl   246.80\n",
      "| epoch   3 |  1320/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.42 | ppl   226.84\n",
      "| epoch   3 |  1340/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.40 | ppl   221.35\n",
      "| epoch   3 |  1360/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.44 | ppl   230.09\n",
      "| epoch   3 |  1380/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.47 | ppl   237.27\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.53 | ppl   252.50\n",
      "| epoch   3 |  1420/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.53 | ppl   251.43\n",
      "| epoch   3 |  1440/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.54 | ppl   254.85\n",
      "| epoch   3 |  1460/ 2928 batches | lr 4.51 | ms/batch 16.80 | loss  5.67 | ppl   288.61\n",
      "| epoch   3 |  1480/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.58 | ppl   263.86\n",
      "| epoch   3 |  1500/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.52 | ppl   249.48\n",
      "| epoch   3 |  1520/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.54 | ppl   255.40\n",
      "| epoch   3 |  1540/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.48 | ppl   239.14\n",
      "| epoch   3 |  1560/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.49 | ppl   241.45\n",
      "| epoch   3 |  1580/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.45 | ppl   232.68\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.45 | ppl   233.09\n",
      "| epoch   3 |  1620/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.44 | ppl   231.12\n",
      "| epoch   3 |  1640/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.39 | ppl   220.04\n",
      "| epoch   3 |  1660/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.52 | ppl   249.35\n",
      "| epoch   3 |  1680/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.41 | ppl   224.05\n",
      "| epoch   3 |  1700/ 2928 batches | lr 4.51 | ms/batch 16.90 | loss  5.51 | ppl   246.00\n",
      "| epoch   3 |  1720/ 2928 batches | lr 4.51 | ms/batch 17.10 | loss  5.47 | ppl   238.29\n",
      "| epoch   3 |  1740/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.48 | ppl   239.86\n",
      "| epoch   3 |  1760/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.51 | ppl   247.94\n",
      "| epoch   3 |  1780/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.47 | ppl   236.92\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 17.00 | loss  5.52 | ppl   249.28\n",
      "| epoch   3 |  1820/ 2928 batches | lr 4.51 | ms/batch 16.25 | loss  5.59 | ppl   268.92\n",
      "| epoch   3 |  1840/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.52 | ppl   248.77\n",
      "| epoch   3 |  1860/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.47 | ppl   238.31\n",
      "| epoch   3 |  1880/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.44 | ppl   231.58\n",
      "| epoch   3 |  1900/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.46 | ppl   234.15\n",
      "| epoch   3 |  1920/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.49 | ppl   242.95\n",
      "| epoch   3 |  1940/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.46 | ppl   236.08\n",
      "| epoch   3 |  1960/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.46 | ppl   234.50\n",
      "| epoch   3 |  1980/ 2928 batches | lr 4.51 | ms/batch 16.15 | loss  5.55 | ppl   256.39\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.42 | ppl   225.76\n",
      "| epoch   3 |  2020/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.35 | ppl   210.44\n",
      "| epoch   3 |  2040/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.46 | ppl   234.07\n",
      "| epoch   3 |  2060/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.32 | ppl   203.40\n",
      "| epoch   3 |  2080/ 2928 batches | lr 4.51 | ms/batch 16.20 | loss  5.33 | ppl   206.70\n",
      "| epoch   3 |  2100/ 2928 batches | lr 4.51 | ms/batch 17.30 | loss  5.33 | ppl   206.17\n",
      "| epoch   3 |  2120/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.41 | ppl   223.64\n",
      "| epoch   3 |  2140/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.37 | ppl   215.42\n",
      "| epoch   3 |  2160/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.32 | ppl   203.47\n",
      "| epoch   3 |  2180/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.34 | ppl   208.21\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.37 | ppl   215.37\n",
      "| epoch   3 |  2220/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.46 | ppl   235.91\n",
      "| epoch   3 |  2240/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.45 | ppl   231.76\n",
      "| epoch   3 |  2260/ 2928 batches | lr 4.51 | ms/batch 16.85 | loss  5.46 | ppl   236.03\n",
      "| epoch   3 |  2280/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.51 | ppl   247.30\n",
      "| epoch   3 |  2300/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.58 | ppl   265.40\n",
      "| epoch   3 |  2320/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.52 | ppl   249.41\n",
      "| epoch   3 |  2340/ 2928 batches | lr 4.51 | ms/batch 16.55 | loss  5.48 | ppl   240.93\n",
      "| epoch   3 |  2360/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.42 | ppl   226.83\n",
      "| epoch   3 |  2380/ 2928 batches | lr 4.51 | ms/batch 16.40 | loss  5.37 | ppl   215.56\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 16.80 | loss  5.42 | ppl   225.68\n",
      "| epoch   3 |  2420/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.50 | ppl   243.53\n",
      "| epoch   3 |  2440/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.44 | ppl   229.38\n",
      "| epoch   3 |  2460/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.40 | ppl   222.17\n",
      "| epoch   3 |  2480/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.47 | ppl   236.66\n",
      "| epoch   3 |  2500/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.52 | ppl   248.76\n",
      "| epoch   3 |  2520/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.57 | ppl   261.34\n",
      "| epoch   3 |  2540/ 2928 batches | lr 4.51 | ms/batch 17.20 | loss  5.53 | ppl   252.64\n",
      "| epoch   3 |  2560/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.48 | ppl   240.38\n",
      "| epoch   3 |  2580/ 2928 batches | lr 4.51 | ms/batch 16.45 | loss  5.42 | ppl   225.20\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 16.10 | loss  5.39 | ppl   218.43\n",
      "| epoch   3 |  2620/ 2928 batches | lr 4.51 | ms/batch 16.70 | loss  5.36 | ppl   213.11\n",
      "| epoch   3 |  2640/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.33 | ppl   206.99\n",
      "| epoch   3 |  2660/ 2928 batches | lr 4.51 | ms/batch 16.35 | loss  5.39 | ppl   219.47\n",
      "| epoch   3 |  2680/ 2928 batches | lr 4.51 | ms/batch 17.10 | loss  5.43 | ppl   227.03\n",
      "| epoch   3 |  2700/ 2928 batches | lr 4.51 | ms/batch 16.15 | loss  5.45 | ppl   232.68\n",
      "| epoch   3 |  2720/ 2928 batches | lr 4.51 | ms/batch 16.30 | loss  5.39 | ppl   219.22\n",
      "| epoch   3 |  2740/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.40 | ppl   221.84\n",
      "| epoch   3 |  2760/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.35 | ppl   210.30\n",
      "| epoch   3 |  2780/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.41 | ppl   223.20\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 16.15 | loss  5.50 | ppl   245.83\n",
      "| epoch   3 |  2820/ 2928 batches | lr 4.51 | ms/batch 16.50 | loss  5.42 | ppl   226.59\n",
      "| epoch   3 |  2840/ 2928 batches | lr 4.51 | ms/batch 16.60 | loss  5.31 | ppl   201.92\n",
      "| epoch   3 |  2860/ 2928 batches | lr 4.51 | ms/batch 16.75 | loss  5.28 | ppl   195.81\n",
      "| epoch   3 |  2880/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.31 | ppl   203.25\n",
      "| epoch   3 |  2900/ 2928 batches | lr 4.51 | ms/batch 16.65 | loss  5.31 | ppl   202.43\n",
      "| epoch   3 |  2920/ 2928 batches | lr 4.51 | ms/batch 16.85 | loss  5.37 | ppl   214.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 51.11s | valid loss  5.55 | valid ppl   258.08\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train() # train a model for one epoch\n",
    "    val_loss = evaluate(model, val_data) # evaluate the performance of the trained model\n",
    "    # Print the performance of the trained model on the evaluation dataset\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # Keep the model if the loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step() # Update the scheduler step with \"StepLR\" at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX6eQys0iz_O"
   },
   "source": [
    "Evaluate the model with the test dataset\n",
    "-------------------------------------\n",
    "\n",
    "Apply the best model to check the result with the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "vvrXlHNfiz_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.47 | test ppl   236.89\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_tutorial-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
